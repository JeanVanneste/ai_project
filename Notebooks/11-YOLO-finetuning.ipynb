{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16144bf",
   "metadata": {},
   "source": [
    "# YOLOv8 Fine-tuning Tutorial\n",
    "\n",
    "This notebook demonstrates how to fine-tune a YOLOv8 model using the Ultralytics library with a local dataset composed of images of men and woman. The dataset of only 39 images and a light training already shows noticable capabilities.\n",
    "\n",
    "## Overview\n",
    "- Install required libraries\n",
    "- Load local dataset in YOLO format\n",
    "- Configure YOLOv8 for training\n",
    "- Train the model\n",
    "- Evaluate and test predictions\n",
    "- Export the fine-tuned model\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- CUDA-compatible GPU (recommended for faster training but not a requirement)\n",
    "- Local dataset in YOLO format (images + labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbfea9a",
   "metadata": {},
   "source": [
    "## Introduction to YOLO (You Only Look Once)\n",
    "\n",
    "**YOLO (You Only Look Once)** is a real-time object detection algorithm that approaches detection as a single regression problem. Unlike traditional methods that apply classifiers to different regions multiple times, YOLO processes the entire image in one forward pass, predicting bounding boxes, class probabilities simultaneously or segmenting the image.\n",
    "\n",
    "![YOLO Architecture](https://user-images.githubusercontent.com/26833433/212094133-6bb8c21c-3d47-41df-a512-81c5931054ae.png)\n",
    "\n",
    "The algorithm divides the input image into a grid system where each cell predicts bounding boxes and confidence scores. This single-pass architecture enables processing speeds of up to 100+ frames per second depending on the model variant and hardware, making it suitable for real-time applications.\n",
    "\n",
    "**YOLOv8**, developed by Ultralytics, is the latest iteration in the YOLO family. It features an anchor-free design, updated data augmentation techniques, and modified loss functions. The architecture provides multiple model sizes from nano to extra-large, allowing users to select between speed and accuracy based on their requirements.\n",
    "\n",
    "YOLO is applied across various domains including autonomous vehicles for obstacle detection, surveillance systems for monitoring, sports analytics for tracking, medical imaging for diagnosis assistance, retail automation for inventory management, and wildlife monitoring for conservation purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b702d8a",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39adf22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Install Ultralytics (YOLOv8) and other dependencies\n",
    "#%pip install ultralytics pillow pyyaml matplotlib opencv-python -q\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import yaml\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672819da",
   "metadata": {},
   "source": [
    "## 2. Load Local Dataset\n",
    "\n",
    "We'll use the local \"dataset HF\" folder which contains:\n",
    "- Images (.jpeg files)\n",
    "- Labels (.txt files in YOLO format: `class_id center_x center_y width height`, all normalized to 0-1)\n",
    "- classes.txt (list of class names)\n",
    "\n",
    "Make sure your dataset folder is in the same directory as this notebook.\n",
    "\n",
    "You can annotate your own images with many different, I used https://www.yololabellingtool.com/ to annotate this dataset, take a look a it and **try to annotate one or multiple images before continuing the notebook**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00285a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local dataset: /home/jeanv/dev/ai_project/Notebooks/dataset HF\n",
      "âœ“ Classes found: ['homme', 'femme']\n",
      "âœ“ Number of classes: 2\n",
      "âœ“ Total images found: 39\n",
      "âœ“ Total label files found: 39\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Using local \"dataset HF\" folder\n",
    "DATASET_HF_PATH = Path(\"dataset HF\")\n",
    "PROJECT_DIR = Path(\"./yolo_finetuning\")\n",
    "DATASET_DIR = PROJECT_DIR / \"dataset\"\n",
    "\n",
    "# Create directories\n",
    "PROJECT_DIR.mkdir(exist_ok=True)\n",
    "DATASET_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Using local dataset: {DATASET_HF_PATH.absolute()}\")\n",
    "\n",
    "# Read class names from the local dataset\n",
    "classes_file = DATASET_HF_PATH / \"classes.txt\"\n",
    "if classes_file.exists():\n",
    "    with open(classes_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        class_names = [line.strip() for line in f.readlines()]\n",
    "    print(f\"âœ“ Classes found: {class_names}\")\n",
    "    print(f\"âœ“ Number of classes: {len(class_names)}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ classes.txt not found at {classes_file}\")\n",
    "    class_names = []\n",
    "\n",
    "# Count files in the dataset\n",
    "image_files = list(DATASET_HF_PATH.glob(\"*.jpeg\")) + list(DATASET_HF_PATH.glob(\"*.jpg\"))\n",
    "label_files = [f for f in DATASET_HF_PATH.glob(\"*.txt\") if f.name != \"classes.txt\"]\n",
    "\n",
    "print(f\"âœ“ Total images found: {len(image_files)}\")\n",
    "print(f\"âœ“ Total label files found: {len(label_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a679f453",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset in YOLO Format\n",
    "\n",
    "YOLOv8 requires datasets in a specific format:\n",
    "```\n",
    "dataset/\n",
    "â”œâ”€â”€ train/\n",
    "â”‚   â”œâ”€â”€ images/\n",
    "â”‚   â””â”€â”€ labels/\n",
    "â”œâ”€â”€ valid/\n",
    "â”‚   â”œâ”€â”€ images/\n",
    "â”‚   â””â”€â”€ labels/\n",
    "â””â”€â”€ data.yaml\n",
    "```\n",
    "\n",
    "Labels should be in YOLO format: `class_id center_x center_y width height` (normalized 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c770e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset: 31 train, 8 validation\n",
      "âœ“ Dataset prepared successfully!\n",
      "  Train: 31 images\n",
      "  Valid: 8 images\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def prepare_yolo_dataset_from_local(source_dir, output_dir, split_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Prepare YOLO dataset from local folder\n",
    "    Split into train and validation sets\n",
    "    \"\"\"\n",
    "    # Create directory structure\n",
    "    train_images_dir = output_dir / \"train\" / \"images\"\n",
    "    train_labels_dir = output_dir / \"train\" / \"labels\"\n",
    "    val_images_dir = output_dir / \"valid\" / \"images\"\n",
    "    val_labels_dir = output_dir / \"valid\" / \"labels\"\n",
    "    \n",
    "    for dir_path in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = list(source_dir.glob(\"*.jpeg\")) + list(source_dir.glob(\"*.jpg\"))\n",
    "    \n",
    "    # Shuffle and split\n",
    "    random.seed(42)\n",
    "    random.shuffle(image_files)\n",
    "    split_idx = int(len(image_files) * split_ratio)\n",
    "    \n",
    "    train_files = image_files[:split_idx]\n",
    "    val_files = image_files[split_idx:]\n",
    "    \n",
    "    print(f\"Splitting dataset: {len(train_files)} train, {len(val_files)} validation\")\n",
    "    \n",
    "    # Copy train files\n",
    "    for img_file in train_files:\n",
    "        label_file = img_file.with_suffix('.txt')\n",
    "        \n",
    "        # Copy image\n",
    "        shutil.copy(img_file, train_images_dir / img_file.name)\n",
    "        \n",
    "        # Copy label if exists\n",
    "        if label_file.exists():\n",
    "            shutil.copy(label_file, train_labels_dir / label_file.name)\n",
    "    \n",
    "    # Copy validation files\n",
    "    for img_file in val_files:\n",
    "        label_file = img_file.with_suffix('.txt')\n",
    "        \n",
    "        # Copy image\n",
    "        shutil.copy(img_file, val_images_dir / img_file.name)\n",
    "        \n",
    "        # Copy label if exists\n",
    "        if label_file.exists():\n",
    "            shutil.copy(label_file, val_labels_dir / label_file.name)\n",
    "    \n",
    "    print(f\"âœ“ Dataset prepared successfully!\")\n",
    "    print(f\"  Train: {len(list(train_images_dir.glob('*')))} images\")\n",
    "    print(f\"  Valid: {len(list(val_images_dir.glob('*')))} images\")\n",
    "    \n",
    "    return train_images_dir, val_images_dir\n",
    "\n",
    "# Prepare the dataset\n",
    "train_img_dir, val_img_dir = prepare_yolo_dataset_from_local(DATASET_HF_PATH, DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0beb0833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure verification:\n",
      "  Train images: yolo_finetuning/dataset/train/images\n",
      "  Train labels: yolo_finetuning/dataset/train/labels\n",
      "  Valid images: yolo_finetuning/dataset/valid/images\n",
      "  Valid labels: yolo_finetuning/dataset/valid/labels\n",
      "\n",
      "âœ“ Files in dataset:\n",
      "  Training: 31 images, 31 labels\n",
      "  Validation: 8 images, 8 labels\n"
     ]
    }
   ],
   "source": [
    "# Verify the dataset structure\n",
    "print(\"Dataset structure verification:\")\n",
    "print(f\"  Train images: {DATASET_DIR / 'train' / 'images'}\")\n",
    "print(f\"  Train labels: {DATASET_DIR / 'train' / 'labels'}\")\n",
    "print(f\"  Valid images: {DATASET_DIR / 'valid' / 'images'}\")\n",
    "print(f\"  Valid labels: {DATASET_DIR / 'valid' / 'labels'}\")\n",
    "\n",
    "# Count files\n",
    "train_imgs = len(list((DATASET_DIR / 'train' / 'images').glob('*')))\n",
    "train_lbls = len(list((DATASET_DIR / 'train' / 'labels').glob('*')))\n",
    "val_imgs = len(list((DATASET_DIR / 'valid' / 'images').glob('*')))\n",
    "val_lbls = len(list((DATASET_DIR / 'valid' / 'labels').glob('*')))\n",
    "\n",
    "print(f\"\\nâœ“ Files in dataset:\")\n",
    "print(f\"  Training: {train_imgs} images, {train_lbls} labels\")\n",
    "print(f\"  Validation: {val_imgs} images, {val_lbls} labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44e633f",
   "metadata": {},
   "source": [
    "## 4. Create data.yaml Configuration File\n",
    "\n",
    "The `data.yaml` file tells YOLOv8 where to find the dataset and what classes exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20c6391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration saved to: yolo_finetuning/dataset/data.yaml\n",
      "\n",
      "Dataset configuration:\n",
      "names:\n",
      "- homme\n",
      "- femme\n",
      "nc: 2\n",
      "path: /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/dataset\n",
      "train: train/images\n",
      "val: valid/images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your dataset configuration using the local dataset\n",
    "data_config = {\n",
    "    'path': str(DATASET_DIR.absolute()),  # Root directory\n",
    "    'train': 'train/images',  # Path to training images\n",
    "    'val': 'valid/images',    # Path to validation images\n",
    "    'nc': len(class_names),   # Number of classes from classes.txt\n",
    "    'names': class_names      # Class names from classes.txt\n",
    "}\n",
    "\n",
    "# Save configuration to YAML file\n",
    "yaml_path = DATASET_DIR / 'data.yaml'\n",
    "with open(yaml_path, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"âœ“ Configuration saved to: {yaml_path}\")\n",
    "print(\"\\nDataset configuration:\")\n",
    "print(yaml.dump(data_config, default_flow_style=False, allow_unicode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803c522",
   "metadata": {},
   "source": [
    "## 5. Load Pre-trained YOLOv8 Model\n",
    "\n",
    "YOLOv8 comes in different sizes:\n",
    "- `yolov8n.pt` - Nano (fastest, least accurate)\n",
    "- `yolov8s.pt` - Small\n",
    "- `yolov8m.pt` - Medium\n",
    "- `yolov8l.pt` - Large\n",
    "- `yolov8x.pt` - Extra Large (slowest, most accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df235457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model type: <class 'ultralytics.models.yolo.model.YOLO'>\n",
      "YOLOv8s summary: 129 layers, 11,166,560 parameters, 0 gradients, 28.8 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(129, 11166560, 0, 28.816844800000002)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained YOLOv8 model\n",
    "# Start with yolov8n (nano) for faster training, or use yolov8m/yolov8l for better accuracy\n",
    "model = YOLO('yolov8s.pt')  # Automatically downloads if not present\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "\n",
    "# Display model information\n",
    "model.info()  # Shows model architecture details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640caac8",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Configure training parameters and start fine-tuning.\n",
    "\n",
    "### Key Training Parameters:\n",
    "- `data`: Path to data.yaml\n",
    "- `epochs`: Number of training epochs\n",
    "- `imgsz`: Image size (640 is standard)\n",
    "- `batch`: Batch size (adjust based on GPU memory)\n",
    "- `patience`: Early stopping patience\n",
    "- `lr0`: Initial learning rate\n",
    "- `device`: 'cuda' for GPU or 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76236ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  data: yolo_finetuning/dataset/data.yaml\n",
      "  epochs: 50\n",
      "  imgsz: 640\n",
      "  batch: 16\n",
      "  patience: 10\n",
      "  save: True\n",
      "  device: 0\n",
      "  project: yolo_finetuning\n",
      "  name: yolov8_custom\n",
      "  exist_ok: True\n",
      "  pretrained: True\n",
      "  optimizer: AdamW\n",
      "  lr0: 0.001\n",
      "  weight_decay: 0.0005\n",
      "  warmup_epochs: 3\n",
      "  augment: True\n",
      "  verbose: True\n",
      "\n",
      "âš ï¸ Note: Training may take several hours depending on dataset size and hardware!\n",
      "You can monitor progress in real-time through the console output.\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "training_args = {\n",
    "    'data': str(yaml_path),      # Path to data.yaml\n",
    "    'epochs': 50,                # Number of epochs (adjust based on dataset size)\n",
    "    'imgsz': 640,                # Image size\n",
    "    'batch': 16,                 # Batch size (reduce if GPU memory is limited)\n",
    "    'patience': 10,              # Early stopping patience\n",
    "    'save': True,                # Save checkpoints\n",
    "    'device': 0 if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n",
    "    'project': str(PROJECT_DIR), # Project directory\n",
    "    'name': 'yolov8_custom',     # Experiment name\n",
    "    'exist_ok': True,            # Overwrite existing project\n",
    "    'pretrained': True,          # Use pretrained weights\n",
    "    'optimizer': 'AdamW',        # Optimizer (SGD, Adam, AdamW)\n",
    "    'lr0': 0.001,                # Initial learning rate\n",
    "    'weight_decay': 0.0005,      # Weight decay\n",
    "    'warmup_epochs': 3,          # Warmup epochs\n",
    "    'augment': True,             # Use data augmentation\n",
    "    'verbose': True,             # Verbose output\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for key, value in training_args.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nâš ï¸ Note: Training may take several hours depending on dataset size and hardware!\")\n",
    "print(\"You can monitor progress in real-time through the console output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bcb39cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.233 available ğŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.232 ğŸš€ Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4050 Laptop GPU, 6140MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=yolo_finetuning/dataset/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8_custom, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=yolo_finetuning, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/yolov8_custom, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,136,374 parameters, 11,136,358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2065.2Â±872.0 MB/s, size: 27.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/dataset/train/labels.cache... 31 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 60.8Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1278.5Â±604.0 MB/s, size: 31.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/dataset/valid/labels.cache... 8 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 5.8Kit/s 0.0s0s\n",
      "Plotting labels to /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/yolov8_custom/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/yolov8_custom\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50      3.59G      1.715      3.184      1.947         72        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 1.2s/it 2.4s2.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 4.7it/s 0.2s\n",
      "                   all          8         14      0.418      0.812      0.533      0.219\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/50      3.47G      1.622      3.156      1.814         74        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.2it/s 0.6s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 10.8it/s 0.1s\n",
      "                   all          8         14      0.401      0.812      0.513      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/50      3.47G      1.751      3.437       2.05         41        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 12.6it/s 0.1s\n",
      "                   all          8         14       0.37      0.822      0.492      0.206\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/50      3.51G        1.8      3.025      2.017         61        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.8it/s 0.7s1.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 10.6it/s 0.1s\n",
      "                   all          8         14      0.543      0.938      0.696      0.392\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/50       3.6G      1.486      1.728      1.605         78        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s1.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 9.7it/s 0.1s\n",
      "                   all          8         14      0.565       0.84       0.63      0.364\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/50      3.61G      1.462      1.744      1.505         67        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.7s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 8.7it/s 0.1s\n",
      "                   all          8         14      0.491      0.964      0.631      0.389\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/50      3.56G      1.224      1.334      1.398         70        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s1.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 8.5it/s 0.1s\n",
      "                   all          8         14      0.633      0.925      0.804      0.516\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/50      3.61G      1.204      1.311      1.414         48        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.2it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 9.4it/s 0.1s\n",
      "                   all          8         14      0.584      0.719      0.721      0.434\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/50      3.58G      1.273      1.439      1.478         59        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.5it/s 0.6s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 14.7it/s 0.1s\n",
      "                   all          8         14      0.556      0.815      0.628      0.369\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/50      3.65G      1.242       1.33      1.419         51        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.5it/s 0.6s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 12.1it/s 0.1s\n",
      "                   all          8         14      0.333      0.909      0.572       0.33\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/50      3.66G        1.2      1.189      1.353         57        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.7s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 9.1it/s 0.1s\n",
      "                   all          8         14      0.333      0.719        0.6      0.354\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/50      3.66G      1.017      1.087      1.197         72        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 8.8it/s 0.1s\n",
      "                   all          8         14      0.628       0.75      0.732       0.44\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/50      3.65G      1.062     0.9983      1.252         64        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 13.5it/s 0.1s\n",
      "                   all          8         14      0.908      0.716       0.88      0.521\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/50      3.66G      1.124      1.079      1.329         61        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.2it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 11.5it/s 0.1s\n",
      "                   all          8         14      0.738      0.897      0.854      0.517\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/50      3.66G     0.9846      1.035      1.232         63        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 10.1it/s 0.1s\n",
      "                   all          8         14      0.706      0.661      0.666      0.388\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/50      3.65G     0.9516     0.9682      1.235         63        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.7it/s 0.7s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 11.4it/s 0.1s\n",
      "                   all          8         14      0.706      0.663      0.594      0.314\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/50      3.66G     0.9286       0.87      1.163         61        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 12.2it/s 0.1s\n",
      "                   all          8         14      0.712       0.75      0.686      0.323\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/50      3.66G      1.106      1.056      1.273         80        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 14.1it/s 0.1s\n",
      "                   all          8         14      0.622      0.781      0.775      0.437\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/50      3.65G     0.9682      0.868      1.262         58        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 11.0it/s 0.1s\n",
      "                   all          8         14      0.715      0.739      0.685      0.426\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/50      3.66G      0.911     0.7317      1.154         68        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.5it/s 0.6s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 13.6it/s 0.1s\n",
      "                   all          8         14      0.468      0.604      0.537       0.31\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/50      3.65G     0.9367     0.8255      1.249         49        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.5it/s 0.6s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 12.4it/s 0.1s\n",
      "                   all          8         14       0.53      0.603      0.553      0.265\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/50      3.66G     0.7983     0.8347      1.142         62        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.6it/s 0.6s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 10.8it/s 0.1s\n",
      "                   all          8         14      0.699      0.521      0.448      0.263\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/50      3.65G     0.9694     0.8969        1.2         59        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 13.7it/s 0.1s\n",
      "                   all          8         14      0.607      0.521      0.538      0.315\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/50      3.66G      0.923     0.8557      1.192         65        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 12.9it/s 0.1s\n",
      "                   all          8         14      0.711      0.598       0.62       0.35\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/50      3.65G     0.9276     0.8647      1.189         75        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 10.5it/s 0.1s\n",
      "                   all          8         14      0.781      0.604      0.776      0.392\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/50      3.66G     0.9356     0.7612      1.199         80        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.2it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 11.6it/s 0.1s\n",
      "                   all          8         14      0.723      0.682      0.851       0.47\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/50      3.67G     0.8231     0.7161      1.118         70        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 12.0it/s 0.1s\n",
      "                   all          8         14      0.542      0.901      0.837      0.527\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/50      3.61G     0.9071       0.75      1.177         63        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.6it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 14.0it/s 0.1s\n",
      "                   all          8         14      0.542      0.901      0.837      0.527\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/50      3.64G     0.7335     0.6785      1.106         74        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.5it/s 0.8s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 11.1it/s 0.1s\n",
      "                   all          8         14      0.528      0.917      0.849      0.486\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/50      3.65G     0.8681      0.759      1.173         53        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.7it/s 0.5s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 14.1it/s 0.1s\n",
      "                   all          8         14      0.566      0.804      0.762      0.435\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      31/50      3.62G     0.8078     0.6686      1.085         76        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.8it/s 0.7s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 10.7it/s 0.1s\n",
      "                   all          8         14      0.566      0.804      0.762      0.435\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      32/50      3.64G     0.8078     0.6332      1.073         67        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 14.7it/s 0.1s\n",
      "                   all          8         14      0.521      0.737      0.734      0.422\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      33/50      3.66G      0.858     0.6881      1.142         61        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.6it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 13.2it/s 0.1s\n",
      "                   all          8         14      0.668      0.604      0.606      0.312\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      34/50       3.6G     0.7591     0.5992      1.056         69        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.7it/s 0.5s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 13.7it/s 0.1s\n",
      "                   all          8         14      0.668      0.604      0.606      0.312\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      35/50      3.61G     0.7261     0.6292      1.052         55        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.6it/s 0.6s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 11.2it/s 0.1s\n",
      "                   all          8         14      0.713      0.604      0.709      0.416\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      36/50      3.65G     0.6732     0.6046      1.066         51        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.2it/s 0.6s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 13.5it/s 0.1s\n",
      "                   all          8         14      0.672      0.625      0.739      0.449\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      37/50      3.75G      0.702     0.6341      1.071         54        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s1.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 13.7it/s 0.1s\n",
      "                   all          8         14      0.672      0.625      0.739      0.449\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      38/50      3.64G     0.7286     0.6332      1.099         55        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 11.9it/s 0.1s\n",
      "                   all          8         14      0.764      0.755      0.797      0.473\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      39/50      3.65G     0.6725     0.6452      1.051         70        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s1.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 12.5it/s 0.1s\n",
      "                   all          8         14      0.792      0.769      0.818      0.418\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      40/50      3.62G     0.6544     0.5726      1.053         65        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.7it/s 0.5s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 12.1it/s 0.1s\n",
      "                   all          8         14      0.792      0.769      0.818      0.418\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      41/50      3.63G     0.6085     0.7426      1.024         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 1.3it/s 1.6s1.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 8.0it/s 0.1s\n",
      "                   all          8         14      0.746      0.669      0.712      0.388\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      42/50      3.63G     0.6083     0.6827      1.011         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s1.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 11.8it/s 0.1s\n",
      "                   all          8         14      0.702      0.854      0.759      0.381\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      43/50       3.6G     0.6214     0.5567      1.023         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 9.9it/s 0.1ss\n",
      "                   all          8         14      0.702      0.854      0.759      0.381\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      44/50      3.62G     0.5362     0.5347      1.039         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.2it/s 0.6s1.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 11.7it/s 0.1s\n",
      "                   all          8         14      0.766      0.676       0.58      0.246\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      45/50       3.6G     0.5153     0.5119     0.9627         26        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.7s1.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 9.9it/s 0.1ss\n",
      "                   all          8         14      0.766      0.676       0.58      0.246\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      46/50      3.62G     0.5779     0.5661      1.004         26        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.7s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 12.8it/s 0.1s\n",
      "                   all          8         14      0.729       0.66      0.554      0.221\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      47/50       3.6G     0.4935     0.5311     0.9635         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s1.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 7.8it/s 0.1s\n",
      "                   all          8         14      0.729       0.66      0.554      0.221\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      48/50      3.62G     0.5109     0.5104     0.9616         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 7.6it/s 0.1s\n",
      "                   all          8         14      0.709       0.66      0.571      0.241\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      49/50       3.6G     0.4949     0.4983     0.9698         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s0.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 10.4it/s 0.1s\n",
      "                   all          8         14      0.709       0.66      0.571      0.241\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      50/50      3.63G     0.5138     0.5048     0.9509         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 10.9it/s 0.1s\n",
      "                   all          8         14      0.709      0.665      0.564      0.265\n",
      "\n",
      "50 epochs completed in 0.023 hours.\n",
      "Optimizer stripped from /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/yolov8_custom/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/yolov8_custom/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/yolov8_custom/weights/best.pt...\n",
      "Ultralytics 8.3.232 ğŸš€ Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4050 Laptop GPU, 6140MiB)\n",
      "Model summary (fused): 72 layers, 11,126,358 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.5it/s 0.3s\n",
      "                   all          8         14      0.542      0.901      0.837      0.527\n",
      "                 homme          6          6      0.706      0.803      0.785      0.546\n",
      "                 femme          6          8      0.379          1      0.889      0.508\n",
      "Speed: 1.0ms preprocess, 23.7ms inference, 0.0ms loss, 3.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/yolov8_custom\u001b[0m\n",
      "âœ“ Training completed!\n",
      "Results saved to: yolo_finetuning/yolov8_custom\n"
     ]
    }
   ],
   "source": [
    "# Start training with the local dataset\n",
    "# Uncomment the following lines when you're ready to train\n",
    "results = model.train(\n",
    "    data=str(yaml_path),\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    project=str(PROJECT_DIR),\n",
    "    name='yolov8_custom',\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training completed!\")\n",
    "print(f\"Results saved to: {PROJECT_DIR / 'yolov8_custom'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44740eb1",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model\n",
    "\n",
    "After training, evaluate the model's performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75970bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fine-tuned model from: yolo_finetuning/yolov8_custom/weights/best.pt\n",
      "Ultralytics 8.3.232 ğŸš€ Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4050 Laptop GPU, 6140MiB)\n",
      "Model summary (fused): 72 layers, 11,126,358 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1997.2Â±615.1 MB/s, size: 26.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/dataset/valid/labels.cache... 8 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 7.5Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.3s/it 1.3s\n",
      "                   all          8         14      0.542      0.901      0.837      0.527\n",
      "                 homme          6          6      0.705      0.802      0.785      0.546\n",
      "                 femme          6          8      0.379          1      0.889      0.508\n",
      "Speed: 1.0ms preprocess, 15.3ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/jeanv/dev/ai_project/Notebooks/runs/detect/val3\u001b[0m\n",
      "\n",
      "==================================================\n",
      "Validation Metrics\n",
      "==================================================\n",
      "mAP50: 0.8372\n",
      "mAP50-95: 0.5269\n",
      "Precision: 0.5419\n",
      "Recall: 0.9009\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the best trained model\n",
    "# After training, the best model is saved as 'best.pt'\n",
    "best_model_path = PROJECT_DIR / 'yolov8_custom' / 'weights' / 'best.pt'\n",
    "\n",
    "# Check if model exists\n",
    "if best_model_path.exists():\n",
    "    model = YOLO(str(best_model_path))\n",
    "    print(f\"Loaded fine-tuned model from: {best_model_path}\")\n",
    "    \n",
    "    # Validate the model on the validation set\n",
    "    metrics = model.val(data=str(yaml_path))\n",
    "    \n",
    "    # Display metrics\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Validation Metrics\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"mAP50: {metrics.box.map50:.4f}\")        # mAP at IoU=0.50\n",
    "    print(f\"mAP50-95: {metrics.box.map:.4f}\")       # mAP at IoU=0.50:0.95\n",
    "    print(f\"Precision: {metrics.box.mp:.4f}\")       # Mean precision\n",
    "    print(f\"Recall: {metrics.box.mr:.4f}\")          # Mean recall\n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    print(f\"Model not found at: {best_model_path}\")\n",
    "    print(\"Please train the model first!\")\n",
    "    print(\"\\nFor demonstration, you can load the pre-trained model:\")\n",
    "    model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae8c8f5",
   "metadata": {},
   "source": [
    "## 8. Make Predictions on New Images\n",
    "\n",
    "Test the fine-tuned model on the validation images. **Then check the result in the runs/detect/predict folders!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5ae5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 validation images\n",
      "Starting predictions on all validation images...\n",
      "\n",
      "\n",
      "image 1/8 /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/dataset/valid/images/OIP-1562483083.jpeg: 480x640 3 femmes, 92.1ms\n",
      "image 2/8 /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/dataset/valid/images/OIP-2079919929.jpeg: 640x448 1 homme, 2 femmes, 87.6ms\n",
      "image 3/8 /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/dataset/valid/images/OIP-2244974167.jpeg: 448x640 1 homme, 2 femmes, 76.4ms\n",
      "image 4/8 /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/dataset/valid/images/OIP-2511001431.jpeg: 448x640 2 hommes, 1 femme, 25.9ms\n",
      "image 5/8 /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/dataset/valid/images/OIP-3146282127.jpeg: 640x448 2 hommes, 2 femmes, 28.6ms\n",
      "image 6/8 /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/dataset/valid/images/OIP-4183490321.jpeg: 384x640 1 homme, 2 femmes, 55.2ms\n",
      "image 7/8 /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/dataset/valid/images/OIP-498467289.jpeg: 640x640 1 homme, 3 femmes, 32.3ms\n",
      "image 8/8 /home/jeanv/dev/ai_project/Notebooks/yolo_finetuning/dataset/valid/images/OIP-647191099.jpeg: 640x480 2 femmes, 55.6ms\n",
      "Speed: 2.4ms preprocess, 56.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/jeanv/dev/ai_project/Notebooks/runs/detect/predict3\u001b[0m\n",
      "âœ“ Predictions complete for 8 images!\n",
      "Results saved to runs/detect/predict\n",
      "\n",
      "Summary:\n",
      "  Total images processed: 8\n",
      "  Total objects detected: 25\n",
      "  Average detections per image: 3.12\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on all validation images\n",
    "test_image_path = DATASET_DIR / 'valid' / 'images'\n",
    "\n",
    "# Get all validation images\n",
    "val_images_list = list(test_image_path.glob('*.jpeg')) + list(test_image_path.glob('*.jpg'))\n",
    "\n",
    "if val_images_list:\n",
    "    print(f\"Found {len(val_images_list)} validation images\")\n",
    "    print(\"Starting predictions on all validation images...\\n\")\n",
    "    \n",
    "    # Make predictions on all images at once\n",
    "    results = model.predict(\n",
    "        source=str(test_image_path),  # Pass the directory path\n",
    "        conf=0.25,          # Confidence threshold\n",
    "        iou=0.45,           # NMS IoU threshold\n",
    "        save=True,          # Save results to disk\n",
    "        show_labels=True,   # Show labels\n",
    "        show_conf=True,     # Show confidence scores\n",
    "        line_width=2,       # Line width of bounding boxes\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Predictions complete for {len(results)} images!\")\n",
    "    print(f\"Results saved to runs/detect/predict\")\n",
    "    \n",
    "    # Summary of detections\n",
    "    total_detections = sum(len(r.boxes) for r in results)\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Total images processed: {len(results)}\")\n",
    "    print(f\"  Total objects detected: {total_detections}\")\n",
    "    print(f\"  Average detections per image: {total_detections/len(results):.2f}\")\n",
    "else:\n",
    "    print(\"No validation images found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a403f",
   "metadata": {},
   "source": [
    "## 11. Training Tips & Best Practices\n",
    "\n",
    "### Dataset Preparation\n",
    "1. **Quality over Quantity**: Ensure your annotations are accurate\n",
    "2. **Balanced Classes**: Try to have roughly equal samples per class\n",
    "3. **Diverse Images**: Include various lighting, angles, and backgrounds\n",
    "4. **Data Augmentation**: YOLOv8 applies augmentation automatically\n",
    "\n",
    "### Training Optimization\n",
    "1. **Start Small**: Use `yolov8n` for quick experiments\n",
    "2. **Monitor Training**: Watch for overfitting (validation loss increasing)\n",
    "3. **Early Stopping**: Use patience parameter to stop when not improving\n",
    "4. **Learning Rate**: Start with default, adjust if needed\n",
    "5. **Batch Size**: Larger batch = more stable, but needs more GPU memory\n",
    "\n",
    "### Common Issues\n",
    "- **Low mAP**: More data, better annotations, longer training\n",
    "- **Overfitting**: More data augmentation, reduce epochs, add regularization\n",
    "- **Out of Memory**: Reduce batch size or image size\n",
    "- **Slow Training**: Use GPU, reduce image size, smaller model\n",
    "\n",
    "### Dataset Sources\n",
    "- **Roboflow Universe**: Pre-labeled datasets in YOLO format\n",
    "- **Hugging Face**: Various computer vision datasets\n",
    "- **COCO, Pascal VOC**: Standard benchmarks\n",
    "- **Custom**: Label your own with tools like LabelImg, CVAT, or Roboflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
